\chapter{Numerical Linear Algebra}\label{ch:linear_algebra}
\begin{quote}
    {\it You cannot learn too much
    linear algebra.} \\ -- Every mathematician
\end{quote}

The preceding comment says it all -- linear algebra is the most important of all of the
mathematical tools that you can learn and build.  The theorems, proofs, conjectures, and
big ideas in almost every other mathematical field find their roots in linear algebra.
Our goal in this chapter is to explore numerical algorithms for the primary questions of
linear algebra: solving systems of equations, approximating solutions to over-determined
and under-determined systems of equations, the eigenvalue-eigenvector problem, and the
singular value problem. Take careful note, that in our current digital age numerical
linear algebra and its fast algorithms are behind the scenes for wide varieties of
computing applications. 


\section{Matrix Operations}
We start this chapter with the basics: the dot product and matrix multiplication.  
\ifnum\Python=0 
MATLAB is designed 
\else
The numerical routines in Python's \texttt{numpy} packages are designed 
\fi
to do these tasks in very efficient ways but it is a good coding exercise to build your
own dot product and matrix multiplication routines.  You'll find in numerical linear
algebra that the indexing and the housekeeping in the codes is the hardest part, so why
don't we start ``easy''.

\begin{problem}
    Recall that the dot product of two vectors $\bu, \bv \in \mathbb{R}^n$ is 
    \[ \bu \cdot \bv = \sum_{j=1}^n u_j v_j. \]
    Without summation notation,
    \[ \bu \cdot \bv = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n. \]
    Write a \ProgLang function that accepts two vectors and returns the dot product. \\
    \ifnum\Python=0
    \mcode{function DotProduct = MyDotProduct(u, v)} \\
    \else
    \mcode{def MyDotProduct(u, v):} \\
    \fi
    It would be wise to put an error check in your code to make sure that the vectors are
    the same size.  You should be able to write this code without any loops.
\end{problem}

\begin{problem}
    Write a test script for the dot product of two random $n \times 1$ vectors.  Your
    script should output the absolute error between your dot product code and \ProgLang's
    \ifnum\Python=0
    \mcode{dot}
    \else
    \mcode{numpy.dot}
    \fi
    command.
\end{problem}

\begin{problem}\label{prob:matrix_mult}
    Recall that if $A$ and $B$ are matrices with $A \in \mathbb{R}^{n \times p}$ and $B \in \mathbb{R}^{p \times m}$
    then the product $AB$ is defined as
    \[ \left( AB \right)_{ij} = \sum_{k=1}^p A_{ik} B_{kj}. \]
    A moments reflection reveals that each entry in the matrix product is actually a dot
    product, 
    \[ \left( AB \right)_{ij} = \left( \text{Row $i$ of matrix $A$} \right) \cdot \left(
    \text{Column $j$ of matirx $B$} \right). \]
    Write a \ProgLang function that accepts two matrices and returns the matrix product. \\
    \ifnum\Python=0
    \mcode{function MatrixProduct = MatrixMultiply(A, B)} \\
    \else
    \mcode{def MatrixMultiply(A, B):} \\
    \fi
    You
    should be able to write this code with only two loops; one for $i$ and one for $j$.
    The rest of the work should be done using dot products.
\end{problem}
\begin{problem}
    Write a test script for the matrix product of two random matrices of appropriate
    sizes.  Your script should output the normed error between your matrix product code
    and \ProgLang's matrix multiplication. \ifnum\Python=1 (Remember to cast your
    \texttt{numpy} arrays as matrices in order to get proper matrix multiplication in
    Python.)\fi
\end{problem}

If you're having trouble seeing that matrix multiplication is just a bunch of dot products
then let's examine a fairly simple example.  You are welcome to use this example to test
your code in the previous problem, but be sure that your code is robust enough to accept
matrices of any size.
\begin{example}
    Find the product of matrices $A$ and $B$ using dot products.
    \[ A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{pmatrix} \qquad B =
    \begin{pmatrix} 7 & 8 & 9 \\ 10 & 11 & 12 \end{pmatrix} \]
    {\bf Solution:} \\
    The product $AB$ will clearly be a $3 \times 3$ matrix since $A \in \mathbb{R}^{3
    \times 2}$ and $B \in \mathbb{R}^{2\times3}$.  To build the matrix product we'll first
    write matrix $A$ as a matrix filled with row vectors and matrix $B$ as a
    matrix filled with column vectors.  Let ${\bf a}_1 = \begin{pmatrix} 1 & 2
    \end{pmatrix}$, ${\bf a}_2 = \begin{pmatrix} 3 & 4 \end{pmatrix}$,  and ${\bf a}_3 =
    \begin{pmatrix} 5 & 6 \end{pmatrix}$ so that we can write $A$ as
    \[ A = \begin{pmatrix} {\bf a}_1 \\ {\bf a}_2 \\ {\bf a}_3
    \end{pmatrix}. \]
    Similarly, write ${\bf b}_1 = \begin{pmatrix} 7\\10\end{pmatrix}$, ${\bf b}_2 =
    \begin{pmatrix} 8 \\ 11\end{pmatrix}$, and ${\bf b}_3 = \begin{pmatrix}
        9\\12\end{pmatrix}$ so that we can write $B$ as
    \[ B = \begin{pmatrix} {\bf b}_1 & {\bf b}_2 & {\bf b}_3 \end{pmatrix}. \]

    Now to build the matrix multiplication we see that $AB$ is given as
    \[ AB = \begin{pmatrix} 
            {\bf a}_1 \cdot {\bf b}_1 & {\bf a}_1 \cdot {\bf b}_2 & {\bf a}_1 \cdot {\bf b}_3 \\
            {\bf a}_2 \cdot {\bf b}_1 & {\bf a}_2 \cdot {\bf b}_2 & {\bf a}_2 \cdot {\bf b}_3 \\
            {\bf a}_3 \cdot {\bf b}_1 & {\bf a}_3 \cdot {\bf b}_2 & {\bf a}_3 \cdot {\bf b}_3
    \end{pmatrix} \]
    Therefore,
    \[ AB = \begin{pmatrix} 27 & 30 & 33 \\
            61 & 68 & 75 \\
        95 & 106 & 117 \end{pmatrix} \]
\end{example}

% \ifnum\Python=1 {\color{red} Note: Write a similar paragraph for Python} \fi
The following \ProgLang code gives the matrix product between two matrices, but when you
build your own code you need to have sufficient catches that avoid inappropriately sized
matrices.
\ifnum\Python=0
\begin{lstlisting}
AB = zeros( size(A,1) , size(B,2) );
for i = 1:size(A,1) % row index
    for j = 1:size(B,2) % column index
        AB(i,j) = dot( A(i,:) , B(:,j) );
    end
end
\end{lstlisting}
\else
\begin{lstlisting}
AB = np.zeros( (A.shape[0], B.shape[1]) )
for i in range(0,A.shape[0]): # row index
    for j in range(0,B.shape[1]): # column index
        AB[i,j] = np.dot( A[i,:] , B[:,j])
\end{lstlisting}
\fi

\begin{problem}
    In matrix arithmetic there are two types of multiplication: matrix-matrix
    multiplication and scalar multiplication.  Modify your code written in problem
    \ref{prob:matrix_mult} so that if one of the two matrices is entered as a scalar (a $1
    \times 1$ matrix) then your \mcode{MatrixMultiply} code gives the correct result. You
    should be able to do this with no loops.
\end{problem}


\newpage\section{Efficiently Solving Systems of Linear Equations}
One of the many classic problems of linear algebra is to solve the linear system $A \bx =
\bb$. In this chapter we will talk about efficient ways to have the computer solve
these systems. You likely recall row reduction (AKA Gaussian Elimination or RREF) from previous linear algebra
courses, but the algorithm that you used is actually slow an cumbersome for computer
implementation.  Even so, let's blow the dust off of what you recall with a small practice
problem.

\begin{problem}
    Solve the following problem by hand using Gaussian Elimination (row reduction).
    \[ \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 0 \end{pmatrix} \begin{pmatrix}
            x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2\end{pmatrix} \]
    Hint: Start by augmenting the coefficient matrix and the right-hand column vector to
    build the augmented system
    \[ \left( \begin{array}{ccc|c} 1 & 2 & 3 & 1 \\ 4 & 5 & 6 & 0 \\ 7 & 8 & 0 & 2
    \end{array} \right). \]
    Then perform row operations to get to the reduced row echelon form
    \[ \left( \begin{array}{ccc|c} 1 & 0 & 0 & \star \\
        0 & 1 & 0 & \star \\
        0 & 0 & 1 & \star \end{array} \right) \]
\end{problem}


\subsection{Lower Triangular Systems}
Row reduction works well on dense (or nearly dense) matrices but if the coefficient matrix
has special structure then we can avoid row reduction in lieu of faster algorithms.
Furthermore, the process that you know as Gaussian Elimination is really just a collection
of sneaky matrix operations.  In the
following two problems you will devise algorithms for triangular matrices.  After we know
how to work with triangular matrices we'll build a general tool for doing Gaussian
Elimination that is easily implemented in a computer.

\begin{problem}
    Outline a fast algorithm (without formal row reduction) for solving the lower triangular system
    \[ \begin{pmatrix} 1 & 0 & 0 \\ 4 & 1 & 0 \\ 7 & 2 & 1 \end{pmatrix} \begin{pmatrix}
        y_1 \\ y_2 \\ y_3 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2\end{pmatrix}. \]
    As a convention we will always write our lower triangular matrices with ones on the
    main diagonal.  Your outline should be a list of explicit steps to solve the system.
    The most natural algorithm that most people devise here is called {\it forward
    substitution}.
\end{problem}


\begin{technique}[Forward Substitution: \texttt{LSolve}]\label{tech:lsolve}
    The following code solves the problem $L {\bf y} = \bb$ using forward
    substitution.  The matrix $L$ is assumed the be lower triangular with ones on the main
    diagonal.

\bcode
\ifnum\Python=0
\begin{lstlisting}
function y = LSolve(L , b)
n = length(b);
y = zeros(n,1);
for i = 1:n
    y(i) = b(i);
    for j = 1 : (i-1)
        y(i) = y(i) - L(i,j) * y(j);
    end
end
\end{lstlisting}
\else
\begin{lstlisting}
def LSolve(L, b):
    L = np.matrix(L) # make sure L is the correct data type
    n = b.size
    y = np.matrix( np.zeros( (n,1)) )
    for i in range(0,n):
        y[i] = b[i]
        for j in range(0,i):
            y[i] = y[i] - L[i,j] * y[j]
    return(y)
\end{lstlisting}
\fi
Take note in the code that we are leveraging the fact that \ProgLang will not enter a
\mcode{for} loop if the ending value is less than the starting value.
\end{technique}

\begin{problem}
    Consider the lower triangular system 
    \[ \begin{pmatrix} 1 & 0 & 0 \\ 4 & 1 & 0 \\ 7 & 2 & 1 \end{pmatrix} \begin{pmatrix}
        y_1 \\ y_2 \\ y_3 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2\end{pmatrix}. \]
    Work the code from Technique \ref{tech:lsolve} \underline{by hand} to solve the
    system.  Keep track of all of the indices as you work through the code.
\end{problem}

\begin{problem}
    Copy the code from Technique \ref{tech:lsolve} into a \ProgLang function but in your code
    write a comment on every line stating what it is doing.  Write a test script that
    creates a lower triangular matrix of the correct form and a right-hand side $\bb$ and
    solve for ${\bf y}$.  Your code needs to work on systems of arbitrarily large size.
\end{problem}


\subsection{Upper Triangular Systems}
Now that we have a method for solving lower triangular systems, let's build a similar
method for solving upper triangular systems.  The merging of lower and upper triangular
systems will play an important role in solving systems of equations.
\begin{problem}
    Outline a fast algorithm (without formal row reduction) for solving the upper triangular system
    \[ \begin{pmatrix} 1 & 2 & 3 \\ 0 & -3 & -6 \\ 0 & 0 & -9 \end{pmatrix}
        \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 1 \\ -4 \\
        3\end{pmatrix} \]
    The most natural algorithm that most people devise here is called {\it backward
    substitution}.  Notice that in our upper triangular matrix we do not have a diagonal
    containing all ones. 
\end{problem}

\begin{technique}[Backward Substitution: \texttt{USolve}]\label{tech:usolve}
    The following code solves the problem $U {\bf x} = {\bf y}$ using backward
    substitution.  The matrix $U$ is assumed the be upper triangular.  You'll notice that
    most of this code is incomplete.  It is your job to complete this code.

\bcode
\ifnum\Python=0
\begin{lstlisting}
function x = USolve(U , y)
n = length(y);
x = zeros(n,1);
for i = ? : ? : ?        % what should we be looping over?
    x(i) = y(i) / ???;   % what should we be dividing by?
    for j = ? : ? : ?    % what should we be looping over? 
        x(i) = x(i) - U(i,j) * x(j) / U(i,i)
    end
end
\end{lstlisting}
\else
\begin{lstlisting}
def USolve(U, y):
    U = np.matrix(U)
    n = y.size
    x = np.matrix( np.zeros( (n,1)))
    for i in range(? , ? , ?):     # what should we be looping over?
        x[i] = y[i] / ???          # what should we be dividing by?
        for j in range(? , ? , ?): # what should we be looping over:
            x[i] = x[i] - U[i,j] * x[j] / U[i,i]
    return(x)
\end{lstlisting}
\fi
\end{technique}

\begin{problem}
    Consider the upper triangular system
    \[ \begin{pmatrix} 1 & 2 & 3 \\ 0 & -3 & -6 \\ 0 & 0 & -9 \end{pmatrix}
        \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 1 \\ -4 \\
        3\end{pmatrix} \]
    Work the code from Technique \ref{tech:usolve} \underline{by hand} to solve the
    system.  Keep track of all of the indices as you work through the code.  You may want
    to work this problem in conjunction with the previous two problems to unpack all of
    the parts of the {\it backward substitution} algorithm.
\end{problem}

\begin{problem}
    Copy the code from Technique \ref{tech:usolve} into a \ProgLang function but in your code
    write a comment on every line stating what it is doing.  Write a test script that
    creates an upper triangular matrix of the correct form and a right-hand side ${\bf y}$ and
    solve for ${\bf x}$.  Your code needs to work on systems of arbitrarily large size.
\end{problem}

\subsection{The LU Factorization}
In the next few problems we will solve the system of equations
\[ \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 0 \end{pmatrix} \begin{pmatrix} x_1
    \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} \]
using upper and lower triangular matrices.  We have already solved this problem with your
Gaussian Elimination algorithm at the beginning of this chapter -- now let's improve upon
that algorithm and reveal some amazing underlying structure.

Throughout the following several problems, let $A$ and $b$ be defined as
\[ A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 0 \end{pmatrix} \qquad \bb =
\begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix}. \]
\begin{problem}
    In \ProgLang enter the matrix $A$ \ifnum\Python=1(Remember to use \mcode{A =
    np.matrix()} to define a Python matrix)\fi.  Observe what happens when we do the
    following in sequence:
    \begin{itemize}
        \item left multiply $A$ by the matrix
            \[ L_1 = \begin{pmatrix} 1 & 0 & 0 \\ -4 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \]
            This gives the matrix $L_1 A$.  (Look at the resulting matrix.  What happened here?)
        \item left multiply $L_1 A$ by the matrix
            \[ L_2 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -7 & 0 & 1 \end{pmatrix} \]
            This gives the matrix $L_2 L_1 A$.  (Look at the resulting matrix.  What happened here?)
        \item left multiply $L_2 L_1 A$ by the matrix 
            \[ L_3 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -2 & 1 \end{pmatrix} \]
            (Look at the resulting matrix.  What happened here?)
    \end{itemize}
\end{problem}

\begin{problem}
    Make a conjecture: If you wanted to multiply row $j$ of an $n\times n$ matrix by $c$
    and add it to row $k$, that is the same as multiplying by what lower triangular
    matrix?
\end{problem}

\begin{problem}
    After the process from the previous problem you should notice that you now have an
    upper triangular matrix.  Hence, in general, we have done this:
    \[ L_3 L_2 L_1 A = U, \]
    so if you solve for $A$ we see that $A$ can be written as
    \[ A = L_1^{-1} L_2^{-1} L_3^{-1} U. \]
    
    Therefore, we could rewrite the problem $A \bx = \bb$ as the
    problem $LU\bx = \bb$ where $L$ is which matrix?
\end{problem}
\solution{
    $L = L_1^{-1} L_2^{-1} L_3^{-1}$
}


\begin{problem}
    In the previous problem you likely found that $L = L_1^{-1} L_2^{-1} L_3^{-1}$.  Use
    \ProgLang to find $L_j^{-1}$ for each $j$ and discuss general observations about how to
    find inverses of lower triangular matrices.  
\end{problem}

\begin{problem}
    Now for the punch line: If we want to solve $A \bx = \bb$ then if we can write it as $LU
    \bx = \bb$ we can
    \begin{enumerate}
        \item Solve $L \by = \bb$ for $\by$ using forward substitution.  Then,
        \item solve $U \bx = \by$ for $\bx$ using backward substitution.
    \end{enumerate}
    For our running example, write down $L$ and $U$ and solve the system.
\end{problem}

\begin{problem}
    Try the process again on the $3\times 3$ system of equations
    \[ \begin{pmatrix}
        3 & 6 & 8\\
        2 & 7 & -1 \\
        5 & 2 & 2 
    \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = 
        \begin{pmatrix} -13 \\ 4 \\ 1 \end{pmatrix} \]
    That is: Find matrices $L$ and $U$ such that $A \bx = \bb$ can be written as $LU\bx =
    \bb$.  Then do two triangular solves to determine $\bx$.\\
    Notice that this time there isn't a ``$1$'' in the top left corner to begin with.  Be
    careful.
\end{problem}


\begin{technique}[LU Factorization]\label{tech:lu}
    The following \ProgLang function takes a square matrix $A$ and
    outputs the matrices $L$ and $U$ such that $A = LU$.  Partial code is given below.
    Complete the code.

\bcode
\ifnum\Python=0
\begin{lstlisting}
function [L,U] = MyLU(A)
n = size(A,1);      % finds the size of the matrix
if size(A,1) ~= size(A,2)   % what does this do?
    fprintf('Error: The matrix A is not square\n')
    break
end
L = eye(n,n);       % initialize L as an identity matrix (why?)
U = A;              % initialize U as A (why?)
for j = 1 : (n-1)   % loop over the columns
    for i = (j+1) : n   % loop over the rows
        mult = A(i,j) / A(j,j); % what does this line do?
        A(i, j+1:n) = A(i, j+1:n) - mult*A(j, j+1:n);
        U(i, j+1:n) = ???   % what part of A should you be putting in U here?
        L(i,j) = ???        % what should go in the lower triangular matrix?
        U(i,j) = 0;         % zero out the bottom portion of U (why?)
    end
end
\end{lstlisting}
\else
\begin{lstlisting}
def MyLU(A):
    n = A.shape[0]
    if A.shape[0] != A.shape[1]:
        print('Error: The matrix A is not square.')
    L = np.matrix( np.identity(n) )
    U = A
    for j in range(0,n-1):
        for i in range(j+1,n):
            mult = A[i,j] / A[j,j]
            A[i, j+1:n] = A[i, j+1:n] - mult * A[j,j+1:n]
            U[i, j+1:n] = A[i, j+1:n]
            L[i,j] = mult
            U[i,j] = 0
    return(L,U)
\end{lstlisting}
\fi
\end{technique}

\begin{thm}[LU Factorization Algorithm]
    Let $A$ be a square matrix in $\mathbb{R}^{n \times n}$ and let $\bx, \bb \in
    \mathbb{R}^n$.  To solve the problem $A \bx =\bb$,
    \begin{enumerate}
        \item Factor $A$ into lower and upper triangular matrices $A = LU$.\\
            \ifnum\Python=0
            \mcode{[L, U] = MyLU(A);}
            \else
            \mcode{L, U = MyLU(A)}
            \fi
        \item The system can now be written as $LU \bx = \bb$.  Substitute $U \bx = {\bf
            y}$ and solve the system $L {\bf y} = \bb$ with forward substitution. \\
            \mcode{y = LSolve(L, b)}
        \item Finally, solve the system $U \bx = {\bf y}$ with backward substitution. \\
            \mcode{x = USolve(U, y)}
    \end{enumerate}
\end{thm}

\begin{problem}
    Test your \mcode{MyLU}, \mcode{LSolve}, and \mcode{USolve} functions on a linear
    system for which you know the answer.  Then test your problem on a system
    that you don't know the solution to.  Discuss where your code will fail. Use the
    following partial code to test your functions.

\bcode
\ifnum\Python=0
\begin{lstlisting}
A = [...; ...; ... ];
b=  [...; ...; ... ];
[L,U] = MyLU(A);
y = Lsolve(L,b);
x = Usolve(U,y)
MATLABExactAnswer = A\b
MyError = norm(x-MATLABExactAnswer)
\end{lstlisting}
\else
\begin{lstlisting}
import numpy as np
A = np.matrix( ) # put the matrix A here
b = np.matrix( ) # put the vector b here
ExactSoln = np.linalg.solve(A,b) # get the exact answer from numpy's linalg.solve()

L, U = MyLU(A) # get the L and U matrices
y = LSolve(L,b) # do the lower solve
x = USolve(U,y) # do the upper solve

Error = np.linalg.norm(ExactSoln - x) # check the size of the difference
print(Error)
\end{lstlisting}
\fi
\end{problem}



\begin{problem}
    For this problem we are going to run a numerical experiment to see how the process of
    solving the equation $A \bx = \bb$ using the $LU$ factorization performs.\\ 
    \noindent Create a loop that does the following 
    \ifnum\Python=1
    \\ (Note to Python Users: You will need to run the import command:\\ \mcode{from
    numpy.random import rand}\\ to get the \mcode{rand} command.)
    \fi
    \begin{itemize}
        \item Build a random matrix of size $n \times n$. You can do this with the code:
            \\
            \mcode{A=rand(n,n)}
        \item Build a random vector in $\mathbb{R}^{n}$. You can do this with the code: \\
            \mcode{b = rand(n,1)}
        \item Find \ProgLang's exact answer to the problem $A\bx=\bb$ 
            \ifnum\Python=0
            using the backslash: \\
            \verb|Xexact = A \ b|
            \else
            using the \mcode{numpy.linalg.solve} command \\
            \mcode{Xexact = numpy.linalg.solve(A,b)}
            \fi
        \item Write code that uses your three $LU$ functions (\mcode{MyLU, Lsolve,
            Usolve}) to find a solution to the equation $A\bx=\bb$.
        \item Find the error between your answer and the exact answer using the code: \\
            \ifnum\Python=0
            \mcode{error = norm(x - Xexact)}
            \else
            \mcode{error = numpy.linalg.norm(x - Xexact)}
            \fi
        \item Make a plot that shows how the error behaves as the size of the problem
            changes. You should run this for matrices of larger and larger size but be
            warned that the loop will run for quite a long time if you go above
            $300 \times 300$ matrices. Just be patient.
    \end{itemize}
\end{problem}

\begin{problem}
    Write a summary of how the \mcode{LU} factorization solve a square system of
    equations.
\end{problem}



\newpage\section{The QR Factorization}
There are several instances where the $LU$ factorization will not perform very fast.  In
fact, there are several ways that the $LU$ factorization (the way that we have stated it)
will fail.  In this section we propose a new factorization that improves the performance
of solving a linear system in some instances.

Let's say that we want to find $\bx$ such that $A \bx =
\bb$ but where $\bb$ is not in the column space of $A$.  Strictly speaking there is no
solution to this type of equation -- the column vector $\bb$ cannot be written as a linear
combination of the columns of $A$.  If we multiply both sides of this equation by $A^T$ we
build what are called the {\bf normal equations:}
\[ A^T A \bx = A^T \bb. \]
In this equation, the right-hand side is a collection of the projections of $\bb$ onto the
column space of $A$.  The nicest type of projections occur when the columns are
perpendicular to each other.  Hence, our goal is to take $A$ and factor it into $A = QR$
where the columns of $Q$ are orthonormal (orthogonal and normalized) and $R$ is an upper
triangular matrix.  In this case we get the following advantages:
\begin{itemize}
    \item If $A \bx = \bb$ then we can write $QR \bx = \bb$.
    \item Multiplying by $Q^T$ on both sides gives $R \bx = Q^T \bb$.
    \item If $R$ is upper triangular then the resulting solve can be done very quickly.
\end{itemize}

\begin{problem}
    If $Q \in \mathbb{R}^{m \times n}$ is an orthonormal matrix then what are the products
    $Q^TQ$ and $QQ^T$?  
\end{problem}

\begin{problem}
    If $A  = QR$ where $Q$ is an orthonormal matrix and $R$ is upper triangular then how
    would we go about solving the equation $A\bx = \bb$ and where would the $QR$
    factorization help us?
\end{problem}

\begin{problem}
    First we'll do a problem by hand: \\
    Consider the matrix $A = \begin{pmatrix} 1 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 1
    \end{pmatrix}$.  We want to factor $A$ into $A = QR$ where the columns of $Q$ are
    orthonormal and $R$ is upper triangular.  Here is the algorithm (run every step by
    hand!).  For notation purposes, $\ba_j$ will be the $j^{th}$ column of $A$.
    \begin{enumerate}
        \item Define $\bq_1 = \displaystyle \frac{\ba_1}{\|\ba_1\|}$.  This will be the
            first column of $Q$.\\
%             \teacher{
%                 \[ \bq_1 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \end{pmatrix} \]
%             }
        \item Define $\bq_2 = \displaystyle \ba_2 - \left( \ba_2 \cdot \bq_1 \right)
            \bq_1$.  Once you've done this calculation normalize your result so
            $\displaystyle \bq_2 = \frac{\bq_2}{\|\bq_2\|}$. This is the second column
            of $Q$.  \\Explain the geometry of this step (DRAW A PICTURE!)\\
%             \teacher{
%                 \[ \bq_2 = \begin{pmatrix} 1\\0\\1\end{pmatrix} - \frac{1}{\sqrt{2}}
%                         \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \\0\end{pmatrix} =
%                             \begin{pmatrix} 1/2 \\ -1/2 \\ 1 \end{pmatrix} \quad \bu_2 =
%                                 \begin{pmatrix} 1/\sqrt{6} \\ -1/\sqrt{6} \\ 2/\sqrt{6}
%                                 \end{pmatrix} \]
%             }
        \item Define $\bq_3 = \displaystyle \ba_3 - \left( \ba_3 \cdot \bq_1 \right)\bq_1
            - \left( \ba_3 \cdot \bq_2 \right)\bq_2$ and then redefine $\displaystyle
            \bq_3 = \frac{\bq_3}{\|\bq_3\|}$.  This is now the third column of
            $Q$.\\
%             \teacher{
%             \[ \bq_3 = \begin{pmatrix} -1/\sqrt{3} \\ 1/\sqrt{3} \\ 1/\sqrt{3}
%                 \end{pmatrix} \]
%             }
        \item The matrix $R$ is formed as follows:
            \[ R = \begin{pmatrix} \ba_1 \cdot \bq_1 & \ba_2 \cdot \bq_1 & \ba_3 \cdot
                    \bq_1 \\ 0 & \ba_2 \cdot \bq_2 & \ba_3 \cdot \bq_2 \\ 0 & 0 & \ba_3
                    \cdot \bq_3 \end{pmatrix} \]
%             \teacher{
%             \[ R = \begin{pmatrix} 2/\sqrt{2} & 1/\sqrt{2} & 1/\sqrt{2} \\ 0 & 3/\sqrt{6}
%                     & 1\/\sqrt{6} \\ 0 & 0 & 2/\sqrt{3} \end{pmatrix} \]
%             }
        \item Write down $Q$ and observe that the process is just begging for several
            loops on a computer to implement this on bigger matrices.
        \item Use \ProgLang to check that $A = QR$.
        \item Finally let's look at the utility of the result:
            \begin{enumerate}
                \item Since $Q$ is an orthonormal matrix $Q^TQ$ is the identity matrix!
                    (Explain why.)
                \item If we want to solve $A\bx = \bb$ and we can write $A = QR$ then 
                    \[ A \bx = \bb \quad \implies \quad QR \bx = \bb \quad \implies \quad
                        R \bx = Q^T \bb \]
                \item Since $R$ is upper triangular we can use the \mcode{Usolve} code we
                    have from our work with the LU-factorization to solve for $\bx$.  THIS
                    IS REALLY FAST!!
                \item Compare this to the number of computer operations needed so solve
                    the normal equations $A^TA\bx = A^T \bb$ with an $LU$-solver.
            \end{enumerate}
    \end{enumerate}
\end{problem}


\begin{problem}
    Write \ProgLang code that takes a matrix $A$ (not necessarily square) and outputs 
    both the $Q$ matrix and the $R$ matrix. Pseducode for this function is as follows:
    \begin{itemize}
        \item Set up the function call:\\ 
            \ifnum\Python=0
            \mcode{function [Q,R] = MyQR(A)}
            \else
            \mcode{def MyQR(A):}
            \fi
        \item Set up \mcode{zeros} matrices for both $Q$ and $R$.  Remember that $A$ may
            not be square so assume that the columns are in $\mathbb{R}^m$ and the rows
            are in $\mathbb{R}^n$.  Hence $Q$ will be $m \times n$ and $R$ will be $n
            \times n$.
        \item Start a loop that counts across the columns:\\ 
            \ifnum\Python=0
            \mcode{for j=1:n}
            \else
            \mcode{for j in range(0,n):}
            \fi
            \begin{itemize}
                \item define a temporary variable: $\hat{\bq} = \ba_j$
                \item start a loop that will do all of the subtractions and build some of
                    the $R's$:\\ 
                    \ifnum\Python=0
                    \mcode{for i=1:j-1}
                    \else
                    \mcode{for i in range(0,j-2):}
                    \fi
                    \begin{itemize}
                        \item build one of the $R's$: $R_{ij} = \ba_j \cdot \bq_i$
                        \item Do one of the subtractions: $\hat{\bq} = \hat{\bq} - R_{ij} \bq_i$
                    \end{itemize}
                \item end the loop for \mcode{i}
                \item normalize the $j^{th}$ column in $Q$: $\bq_j = \hat{\bq} / \| \hat{\bq} \|$
                \item build the $R's$ on the diagonal: $R_{jj} = \ba_j \cdot \bq_j$
            \end{itemize}
        \item end the loop for \mcode{j}
    \end{itemize}
\end{problem}

\begin{problem}
    Write a script that tests your \mcode{MyQR} function on randomly generated $m\times n$ matrices
    with randomly generated right-hand sides. Compare the time that it takes to solve the
    least squares problems using QR to the time necessary to solve with LU via the normal
    equations. When is LU more efficient? When is QR more efficient?
\end{problem}

\newpage\section{Curve Fitting -- The Least Squares Problem via Linear Algebra}\label{sec:least_squares}
\begin{problem}\label{prob:least_squares_1}
    Grab the data sets from the Google Sheet
    \href{https://docs.google.com/spreadsheets/d/1bpqb51eTTtJbe9V1JLt_JkzN-8kbIh2RzisRQHIMLrY/edit?usp=sharing}{HERE}.
    Copy the data into MS Excel for the following exercise.  We're going to develop a way
    to match the data sets using linear algebra.  Before doing the linear algebra versions
    of this problem though we'll use Excel to match the data sets.  Our goal is to make a
    guess at the type of polynomial that models the function (given in this case) and to
    minimize the error between our guess and the data.  Follow these steps in Excel to
    find best fitting curves.  We'll start with the linear data.
    \begin{enumerate}
        \item In column C set up a function for your guess of the model based on the $x$ data in column
            $A$.  You will need to set up cells for the polynomial parameters (for the
            linear case these are the slope and the $y$-intercept).
        \item Fill in your guesses based on initial approximations for the slope and
            $y$-intercept.
        \item Use column D to calculate the residual value for each data point.
            \[ \text{Residual} = \text{Actual $y$ Value} - \text{Approximate $y$ Value} \]
        \item Use column E to calculate the square of the residual for each data point.
        \item Our goal is the minimize the sum of the squares of the residuals.
            \[ \text{min} \left( \sum_{j=1}^n \left( y_j - \hat{y}_j \right)^2 \right) \]
            For this we can use the \texttt{Excel Solver} to minimize the sum of the
            square residuals.
    \end{enumerate}
    Now repeat the process for the quadratic data.
\end{problem}




At this point we need to discuss the quality of the fits that we are building.  You are
likely familiar with the $R$ and $R^2$ values from statistics, but let's just recap here.  
\begin{definition}[Correlation Coefficient, $R$]
    The {\bf correlation coefficient}, $R$, is a measure of the quality of a linear fit.
    \begin{itemize}
        \item If $R = +1$ then the data represent a perfect linear fit with a positive slope. 
        \item If $R = -1$ then the data represent a perfect linear fit with a negative
            slope.  
        \item If $R = 0$ then there is no linear relationship between the two variables.
    \end{itemize}
\end{definition}

\begin{definition}[Coefficient of Determination, $R^2$]
    The {\bf coefficient of determination}, $R^2$, is the proportion of variance in the
    dependent ($y$) variable that is explained by the independent ($x$) variable.  
    \begin{itemize}
        \item If $R^2 = 1$ then 100\% of the variance in $y$ is explained by $x$ and the
            fit is perfect. 
        \item If $R^2 = 0$ then 0\% of the variance in $y$ is explain by $x$ and there is
            no correlation between the two variables.
    \end{itemize}
\end{definition}

In the problems that we've studied thus far we have calculated the sum of the squares of
the residuals as a measure of how well our function fits the data.  The trouble with the
sum of the squares of the residuals is that it is context dependent.  Hence, there is no
way to tell at the outset what a {\it good} sum of squares of residuals is.  

\begin{thm}
    To calculate the $R^2$ value we can use the following equations where $y_j$ is the
    $j^{th}$ data point, $\bar{y}$ is the mean $y$-value in the data, and $\hat{y}_j$ is the
    $j^{th}$ predicted output from our model.
    \begin{flalign}
        \text{Total Sum of Squares: } & TSS = \sum_j \left( y_j - \bar{y} \right)^2
        \label{eqn:TSS} \\
        \text{Residual Sum of Squares: } & RSS = \sum_j \left( y_j - \hat{y}_j \right)^2 
        \label{eqn:RSS} \\
        \text{Coeff. of Determination: } & R^2 = 1 - \frac{RSS}{TSS} \label{eqn:Rsquared}
    \end{flalign}
\end{thm}

\begin{problem}
    Compute the $R^2$ value for each of the data sets from Problem
    \ref{prob:least_squares_1}.
\end{problem}

\begin{problem}
    Use equation \eqref{eqn:Rsquared} to explain why $R^2=1$ implies a perfect fit.
\end{problem}
\solution{
    If $R^2 = 1$ then $RSS = 0$ and this implies that there are no residuals.  Hence we
    have made no errors with our model.
}

There are MANY statistics packages out there that will do least squares regression for us.
\ProgLang has the \texttt{polyfit} command, R has the \texttt{lm} command, and Excel has the
data analysis toolpack.  Behind the scenes in all of these is actually linear algebra.  It
is informative pedagogically to do the least squares problems using Excel a few times (as
we did in Problem \ref{prob:least_squares_1}), but in reality there is some beautifully
simple linear algebra behind the scenes.

\begin{problem}
    Now we'll use Linear Algebra to complete the same types of problems.  Set up a
    \ProgLang script the reads the linear data from the Excel sheet.  We are
    assuming that the data are linear so for each $x$-value $x_j$ we assume that the equation 
    \[ \hat{y}_j =  \beta_0 + \beta_1 x_j \approx y_j. \]
    The values of $\beta_0$ and $\beta_1$ are the intercept and slope that best predict
    the $y$ values from the data.

    The equation $\beta_0 + \beta_1 x_j \approx y_j$ gives rise to a system of linear equations
    \[ \begin{pmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \\ \vdots & \vdots \\ 1 & x_n
        \end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix} \approx \begin{pmatrix} y_1
            \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{pmatrix} \]
    when we consider all $n$ of the data points.
    We'll call the left-hand matrix $A$ and the right-hand vector $\by$.  
    \begin{enumerate}
        \item For each data set get $A$ and $\by$ into \ProgLang so we can use them.
        \item This is not a square system. In fact, it is {\it overdetermined} (more rows
            than columns).  Consider
            that the column space of $A$ ($Col(A)$) is a subspace of $\mathbb{R}^n$ and
            that $\by \in \mathbb{R}^n$ is likely not in the column space of $A$.  If we
            project $\by$ onto $Col(A)$ we should be able to find the best approximation
            of $\by$ that lies in $Col(A)$. 

            Projections onto a subspace can be achieved with matrix multiplication, but
            which matrix shall we multiply by \dots

            \[ (??) A \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}  = (??) \by. \]

            Once you are satisfied that the right-hand side is a projection of $\by$ onto
            $Col(A)$ you have formed the {\bf normal equations}.  If you've done everthing
            right then you also now have a square system ($2 \times 2$ in the case of the
            linear data).

        \item Now that this is a square matrix you can solve for $\beta_0$ and $\beta_1$ using
            your $LU$ or $QR$ code. \ifnum\Python=0 (You can also use \texttt{MATLAB}'s {\it backslash} (\verb|\|) command to
            do the linear solve)\fi
        \item Now that you have $\beta_0$ and $\beta_1$ you can form the linear approximation.
            Plot the approximation along with your data.  Also write code to plot the
            residuals (subplot would be great here).
        \item Repeat the process for the quadratic.  For the quadratic data
            you are assuming that 
            \[ \beta_0 + \beta_1 x_j + \beta_2 x_j^2 \approx y_j. \]
    \end{enumerate}
\end{problem}


\begin{definition}[The Normal Equations]
    Let $A \in \mathbb{R}^{n \times m}$, $\beta \in \mathbb{R}^{m}$, and $\by \in
    \mathbb{R}^n$ where $n \gg m$.  The system of equations
    \[ A \beta = \by \]
    is over determine since there are more equations than unknowns.  Multiply both sides
    of the equation by $A^T$ yields the {\bf normal equations}
    \[ A^T A \beta = A^T \by. \]
    We note here that $A^T A \in \mathbb{R}^{m \times m}$ is square and much smaller than
    $A$.  The right-hand side of the normal equations is the projection of $\by$ onto the
    columns space of $A$.  
\end{definition}

You should take careful note of something here.  The process that we just formulated is
called {\it linear regression} even though we were fitting a quadratic function to data.
This may, at first, seem like an unfortunate or inappropriate naming convention, but stop
and think more carefully about what we did.  \ldots Good.  Now that you've though about
it, I'll give you my take.  

For the quadratic data fit we are trying to create the function $\hat{y}_j = \beta_0 +
\beta_1 x_j + \beta_2 x_j^2$.  Notice that this function is really just a multiple
regression that is, indeed, linear in the coefficients $\beta_0, \beta_1$, and $\beta_2$.
The term {\it linear regression} does \underline{not} have anything to do fitting lines to
data -- a common misconception.  Instead, it pertains to the relationship between the coefficients.
Since the coefficients of a polynomial function are linearly related to
each other, we can use linear regression to fit polynomial models for any order
polynomial.  

The reader should be further warned that polynomial regression comes with some down sides.
If the order of the polynomial grows the matrix $A^TA$ arising from the normal equations
formulation gets closer and closer to being singular.  That is to say that as the order of
the polynomial increases it becomes less and less desirable to solve the problem through
matrix inversion (or Gaussian Elimination).  

\begin{problem}
    In this problem we are going to follow up on the claim in the previous paragraph:
    \begin{quote}
        {\it When doing polynomial regression via the normal equations, as the order of
            the polynomial grows the matrix $A^T A$ gets close and closer to being
        singular.}
    \end{quote}
    \begin{enumerate}
        \item[(a)] Create a data set from a third or fourth order polynomial.  Then
            introduce some random noise onto your data set.  
        \item[(b)] Write a loop in \ProgLang that fits polynomials of increasing order to
            the data set (starting with linear and increasing by 1 each time).  Create two
            plots.  In the first plot put the order of the polynomial on the horizontal
            axis and the $R^2$ value on the vertical axis.  On the second plot put the
            order of the polynomial on the horizontal axis and the ratio $\left|
            \lambda_{max} \right| / \left| \lambda_{min} \right|$ on the vertical axis,
            where $\lambda_{max}$ and $\lambda_{min}$ are the maximum and minimum
            eigenvalues respectively.  You'll probably want to use a \mcode{semilogy}
            scale for the second plot.  
        \item[(c)] What do you notice about the $R^2$ value in the first plot as you
            increase the order of the polynomial?  Is this surprising?  Discuss.
        \item[(d)] What does the right-hand plot tell you about the matrix $A^T A$?
        \item[(e)] Why is the ratio $\lambda_{max}/\lambda_{min}$ a measure of how close
            to singular (non-invertible) the matrix $A^T A$ is? 
    \end{enumerate}
%     Note: Recall that the determinant of a matrix is the product of the eigenvalues.
%     Therefore, if the determinant is getting close to zero then the ratio given above goes
%     to infinity.  To get the eigenvalues use MATLAB's \mcode{eig} command.
\end{problem}

\begin{problem}
    Explain what the previous problem tells you about fitting polynomials to data.
\end{problem}




\newpage\section{The Eigenvalue-Eigenvector Problem}
Recall that the eigenvectors, $\bx$, and the eigenvalues, $\lambda$ of a square matrix
satisfy the equation $A\bx=\lambda \bx$. Geometrically, the eign-problem is the task of
finding the special vectors $\bx$ such that multiplication by the matrix $A$ only produces
a scalar multiple of $\bx$. Thinking about matrix multiplication, this is rather peculiar
since matrix-vector multiplication usually results in a scaling and a rotation of the
vector. Therefore, in some sense the eigenvectors are the only special vectors which avoid
geometric rotation under matrix multiplication.  For a graphical exploration of this idea
see:\\ \href{https://www.geogebra.org/m/JP2XZpzV}{https://www.geogebra.org/m/JP2XZpzV}.

Recall that to solve the eigen-problem for a square matrix $A$ we complete the following
steps:
\begin{enumerate}
    \item First rearrange the definition of the eigenvalue-eigenvector pair to 
        \[ (A\bx-\lambda \bx)=\bo. \]
    \item Next, factor the $\bx$ on the right to get 
        \[ (A-\lambda I) \bx=\bo. \]
    \item Now observe that since $\bx \ne 0$ the matrix $A-\lambda I$ must NOT have an inverse. Therefore,
        \[ \det(A-\lambda I)=0. \]
    \item Solve the equation $\det(A-\lambda I)=0$ for all of the values of $\lambda$.
    \item For each $\lambda$, find a solution to the equation $(A-\lambda I) \bx=\bo$.
        Note that there will be infinitely many solutions so you will need to make wise
        choices for the free variables.
\end{enumerate}
\begin{problem}
    Find the eigenvalues and eigenvectors of $A = \begin{pmatrix} 1 & 2 \\ 4 & 3
    \end{pmatrix}$ \underline{by hand}.
\end{problem}
% \teacher{the eigenvalues of $\lambda_1 = -1$ and $\lambda_2 = 5$ with eigenvectors $\bv_1
%     = (1,-1)^T$ and $\bv_2 = (1,2)^T$
% }

\begin{problem}
    In the matrix $A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix}$
    one of the eigenvalues is $\lambda_1 = 0$.
    \begin{enumerate}
        \item What does that tell us about the matrix $A$?
%             \teacher{$A$ is not invertible. }
        \item What is the eigenvector $\bv_1$ associated with $\lambda_1 = 0$?  
%             \teacher{$\bv_1 = (1,-2,1)^T$}
        \item What is the null space of the matrix $A$?
    \end{enumerate}
\end{problem}

\begin{problem}
    Find matrices $P$ and $D$ such that $A = \begin{pmatrix} 1 & 2 \\ 4 & 3 \end{pmatrix}$
        can be written as $A = PDP^{-1}$ where $P$ is a dense $2 \times 2$ matrix and $D$
        is a diagonal matrix.  Once you have this factorization of $A$, use it to
        determine $A^{10}$.
\end{problem}
% \teacher{
%     \[ P = \begin{pmatrix} 1 & 1 \\ -1 & 2 \end{pmatrix} \quad D = \begin{pmatrix} -1 & 0
%             \\ 0 & 5 \end{pmatrix} \quad P^{-1} = \begin{pmatrix} 2/3 & -1/3 \\ 1/3 & 1/3
%             \end{pmatrix} \]
%             \[ A^{10} = P D^{10} P^{-1} = P \begin{pmatrix} 1 & 0 \\ 0 & 9765625
%                 \end{pmatrix} P^{-1} \]
% }

\begin{problem}
    Let $A$ be an $n \times n$ matrix with $n$ distinct eigenvectors $\bv_1, \bv_2, \dots,
    \bv_n$ and let $\bx \in \mathbb{R}^n$ be a vector such that $\bx = \sum_{j=1}^n c_j
    \bv_j$. Find expressions for $A\bx$, $A^2 \bx$, $A^3\bx$, \dots
\end{problem}
% \teacher{
%     \[ A^k\bx = \sum_{j=1}^n c_j \lambda_j^k \bv_j \]
% }

\begin{problem}
    In this problem  we first describes the mathematical idea for the {\bf power method} for
    computing the largest eigenvalue / eigenvector pair.  Then we write an algorithm for
    find the largest eigen-pair numerically.
    \begin{enumerate}
        \item Assume that $A$ has $n$ linearly independent eigenvectors $\bv_1, \bv_2,
            \dots, \bv_n$ and choose $\bx =
            \sum_{j=1}^n c_j \bv_j$.  From the previous problem,
            \[ A^k \bx = \underline{\hspace{2in}} \]
% \teacher{
%     \[ A^k\bx = \sum_{j=1}^n c_j \lambda_j^k \bv_j \]
% }
        \item Factor the right-hand side so that 
            \[ A^k \bx = \lambda_1^k \left( c_1 \bv_1 + c_2 \left(
                \frac{\lambda_2}{\lambda_1} \right)^k \bv_2 + c_3 \left(
                \frac{\lambda_3}{\lambda_1}
                \right)^k \bv_3 + \cdots + c_n \left( \frac{\lambda_n}{\lambda_1}
                \right)^k \bv_n \right) \]
        \item If $\lambda_1 > \lambda_2 \ge \lambda_3 \ge \cdots \ge \lambda_n$ then what
            happens to each of the $(\lambda_j/\lambda_1)^k$ terms as $k \to \infty$?
            Using this answer, what is $\lim_{k \to \infty} A^k \bx$?\\
%             \teacher{
%                 \[ \lim_{k \to \infty} A^k \bx = \lambda_1^k c_1 \bv_1 \]
%             }
    \end{enumerate}
\end{problem}

    \begin{technique}[The Power Method Algorithm] This algorithm will quickly find the 
        eigenvalue of largest absolute value for a square matrix $A \in \mathbb{R}^{n \times
        n}$ as well as the associated (normalized) eigenvector.  We are
        assuming that there are $n$ linearly independent eigenvectors of $A$.
        \begin{description}
            \item[Step \#1:] Given a nonzero vector $\bx$, set $\bv^{(1)} = \bx / \|\bx\|$.
                (Here the superscript indiates the iteration number)
            \item[Step \#2:] For $k=2, 3, \ldots$
                \begin{description}
                    \item[Step \#2a:] Compute $\tilde{\bv}^{(k)} = A \bv^{(k-1)}$ (this gives
                        a non-normalized version of the next estimate of the dominant
                        eigenvector.)
                    \item[Step \#2b:] Set $\lambda^{(k)} = \left< \tilde{\bv}^{(k)} ,
                        \bv^{(k-1)} \right>$.  (this gives an approximation of the eigenvalue
                        since if $\bv^{(k-1)}$ was the actual eigenvector we would have
                        $\lambda = \left< A \bv^{(k-1)}, \bv^{(k-1)} \right>$)
                    \item[Step \#2c:] Normalize $\tilde{\bv}^{(k)}$ by computing $\bv^{(k)} =
                        \tilde{\bv}^{(k)} / \| \tilde{\bv}^{(k)} \|$. (This guarantees that
                        you will be sending a unit vector into the next iteration of the loop)
                \end{description}
        \end{description}
\end{technique}

\begin{problem}
    Write a \ProgLang function to implement the power method for finding the eigenvalue of
    largest absolute value and the associated eigenvector.  Test it on a matrix where you
    know the eigenvalue of interest.
\end{problem}


\newpage\section{The Singular Value Decomposition}
Our overarching goal of this section is to discuss an analogue to the
eigenvalue-eigenvector problem for non-square matrices.  That is, we would like to take a
matrix $A$ that is $m \times n$ and find vectors are values that behave similarly to how
eigenvectors and eigenvalues behave for square matrices.  The key to this discussion is
the matrix $A^T A$, so let's start there.

\begin{problem}
    Let $A$ be an $m \times n$ matrix.  What is the size of $A^T A$?  Prove that $A^T A$
    must be a symmetric matrix (a matrix $B$ is symmetric if $B_{ij} = B_{ji}$).  Finally, what bearing
    does the next Theorem have on the matrix $A^T A$?
\end{problem}
% \teacher{
%     Let $B = A^T A$.  Hence $B_{ij} = \ba_i \cdot \ba_j = \ba_j \cdot \ba_i = B_{ji}$.
%     Since $B$ is symmetric and real we know that $A^T A$ will have $n$ orthogonal
%     eigenvectors.
% }




\begin{thm}\label{thm:sym_matrix}
    An $n \times n$ matrix $A$ has $n$ orthogonal eigenvectors if and only if $A$ is a
    symmetric matrix.
\end{thm}


\begin{definition}
The {\bf singular values} of an $m \times n$ matrix $A$ are the square roots of the
eigenvalues of $A^T A$.
They are typically denoted as $\sigma_1, \sigma_2, \dots, \sigma_n$ where $\sigma_j =
\sqrt{\lambda_j}$ and $\lambda_j$ is an eigenvalue of $A^T A$.  
\end{definition}

\begin{definition}
    The {\bf singular value decomposition} of an $m \times n$ matrix $A$ with rank $r$ is
    a factorization of $A$ into the product of three matrices, $U$, $\Sigma$, and $V$,
    such that
    \[ A = U \Sigma V^T. \]
    In the singular value decomposition, $U$ ($m \times m$) and $V$ ($n \times n$) have
    orthogonal columns and $\Sigma$ ($m \times n$)
    is a block diagonal matrix 
    \[ \Sigma = \begin{pmatrix} D & 0 \\ 0 & 0 \end{pmatrix} \]
    where $D$ is an $r \times r$ diagonal matrix containing the $r$ singular values of
    $A$ in rank order (largest to smallest).

    To build the singular value decomposition:
    \begin{enumerate}
        \item Form $A^TA$ and find the eigenvalues and eigenvectors (guaranteed to exist
            by Theorem \ref{thm:sym_matrix}).
        \item Form $\Sigma$
        \item The columns of $V$ are the eigenvectors of $A^T A$.
        \item The columns of $U$ are the normalized vectors obtained by 
            \[ \bu_1 = \frac{1}{\sigma_1} A \bv_1\, , \, \bu_2 = \frac{1}{\sigma_2} A
            \bv_2 \, , \, \dots, \, \bu_m = \frac{1}{\sigma_m} A \bv_m \]
    \end{enumerate}
\end{definition}

\begin{problem}
    Use \ProgLang to find the singular value decomposition of 
    \[ A = \begin{pmatrix} 4 & 11 & 14 \\ 8 & 7 & -2 \end{pmatrix} \]


\ifnum\Python=1 {\color{red} Continue altering the coding sections starting here}\fi

    Some practical MATLAB tips follow:
    \begin{enumerate}
        \item Define $A$
        \item Define the sizes: \mcode{m=size(A,1); n=size(A,2)}
        \item Find the rank of $A$: \mcode{r = rank(A);}
        \item Define the matrices \mcode{Sigma} and \mcode{U} to be zero matrices with
            the right size.
        \item Have MATLAB calculate the eigenvectors and eigenvalues of $A^T A$:\\
            \mcode{[vectors,values]=eig(A'A,'vector');}\\
            The \mcode{'vector'} command spits out the eigenvalues as a vector instead of
            a diagonal matrix.  This will be helpful in the next step.
        \item Have MATLAB sort the eigenvalues and strip any negative {\it approximate
            zero} eigenvalues that arise from numerical approximation of zero. \\
            \mcode{values = abs(values);}\\
            \mcode{[values,indices] = sort(values,'descend')}
        \item Sort the columns of $V$ using the indices coming out of the sort command: \\
            \mcode{V = vectors(:,indices);}
        \item Build the singular values from the eigenvalues of $A^TA$ (remember the
            square root!):\\ \mcode{singularvalues=...}
        \item Build non-zero diagonal entries of the $\Sigma$ matrix with a loop.  Also build a
            temporary matrix $B$ the same size as $\Sigma$ but with the diagonal entries
            $1/\sigma_j$.  We'll need $B$ in the next step. 
\begin{lstlisting}
B=zeros(size(Sigma));
for j=1:r}
Sigma(j,j) = ...
B(j,j) = ...
end
\end{lstlisting}
        \item Observe that since $V$ has orthonormal columns we can write $AV = U \Sigma$.
            Now, $\Sigma$ is not square, but we know that it has diagonal entries only so
            we have a {\it pseudo-inverse} $B^T$ already built.  Hence, $U = A V B^T$.
            Build $U$.
        \item Check that $A = U \Sigma V^T$
    \end{enumerate}
\end{problem}

\begin{problem}
    Create a \ProgLang function that accepts a matrix $A$ and outputs the three matrices for
    the singular value decomposition.  Test your function on a large random rectangular
    matrix.
    \ifnum\Python=0
\begin{lstlisting}
A = rand(500,300);
[U,S,V] = MySVD(A);
error = norm(A - U*S*V')
\end{lstlisting}
\else
\begin{lstlisting}
import numpy as np
A = np.random.rand(500,300)
U, S, V = MySVD(A)
error = np.linalg.norm(A - U*S*np.transpose(V))
print(error)
\end{lstlisting}
\fi
\end{problem}






\newpage\section{Exercises}

\subsection{Algorithm Summaries}

\begin{problem}
    Explain in clear language how to efficiently solve an upper triangular system of
    linear equations.
\end{problem}

\begin{problem}
    Explain in clear language how to efficiently solve a lower triangular system of
    linear equations.
\end{problem}

\begin{problem}
    Explain in clear language how to solve the equation $A \bx = \bb$ using an $LU$
    decomposition.  
\end{problem}

\begin{problem}
    Explain in clear language how to solve an overdetermined system of linear equations (more
    equations than unknowns) numerically.
\end{problem}

\begin{problem}
    Explain in clear language the algorithm for finding the columns of the $Q$ matrix in
    the $QR$ factorization.  Give all of the mathematical details.
\end{problem}

\begin{problem}
    Explain in clear language how to find the upper triangular matrix $R$ in the $QR$
    factorization.  Give all of the mathematical details.
\end{problem}

\begin{problem}
    Explain in clear language how to solve the equation $A \bx = \bb$ using a $QR$
    decomposition.
\end{problem}

\begin{problem}
    Explain in clear language how the Power Method works to find the dominant eigenvalue
    and eigenvector of a square matrix.  Give all of the mathematical details.
\end{problem}

\begin{problem}
    Explain in clear language how to find the Singular Value Decomposition of a matrix.
\end{problem}

\subsection{Applying What You've Learned}


\begin{problem}\label{prob:heated_rod_lin_alg}
    Imagine that we have a 1 meter long thin metal rod that has been heated to 100$^\circ$
    on the left-hand side and cooled to 0$^\circ$ on the right-hand side.  We want to know
    the temperature every 10 cm from left to right on the rod.
    \begin{enumerate}
        \item[(a)] First we break the rod into equal 10cm increments as shown.
            \begin{center}
                \begin{tikzpicture}
                    \draw[thick,|-|, black] (0,0) -- (10,0);
                    \foreach \j in {0,1,2,3,4,5,6,7,8,9,10}{
                        \draw[black, thick, fill=black] (\j,0) circle(0.075cm);
                        \draw[black, thick] (\j,0.2) -- (\j,-0.2)
                        node[anchor=north]{$x_{\j}$};
                    }
                \end{tikzpicture}
            \end{center}
            How many unknowns are there in this picture?
        \item[(b)] The temperature at each point along the rod is the average of the
            temperatures at the adjacent points.  For example, if we let $T_1$ be the
            temperature at point $x_1$ then
            \[ T_1 = \frac{T_0 + T_2}{2}. \]
            Write a system of equations for each of the unknown temperatures.
        \item[(c)] Solve the system for the temperature at each unknown node.
    \end{enumerate}
\end{problem}
\solution{
    \begin{flalign*}
        T_1 &= \frac{T_0 + T_2}{2} = \frac{100 + T_2}{2} \\
        T_2 &= \frac{T_1 + T_3}{2} \\
        T_3 &= \frac{T_2 + T_4}{2} \\
        T_4 &= \frac{T_3 + T_5}{2} \\
        T_5 &= \frac{T_4 + T_6}{2} \\
        T_6 &= \frac{T_5 + T_7}{2} \\
        T_7 &= \frac{T_6 + T_8}{2} \\
        T_8 &= \frac{T_7 + T_9}{2} \\
        T_9 &= \frac{T_8 + T_{10}}{2} = \frac{T_8}{2} \\
    \end{flalign*}
    Solving this system we get $T_0=100, T_1 = 90, T_2 = 80, T_3 = 70, \cdots, T_10 = 0$.
}

\begin{problem}
    Write code to solve the following systems of equations via both LU and QR
    decompositions.
    \begin{enumerate}
        \item[(a)] 
            \[ \begin{array}{rl} x + 2y + 3z &= 4 \\ 2x + 4y + 3z &= 5 \\ x + y &= 4
                \end{array} \]
        \item[(b)] 
            \[ \begin{array}{rl} 2y + 3z &= 4 \\ 2x + 3z &= 5 \\ y &= 4
                \end{array} \]
        \item[(b)] 
            \[ \begin{array}{rl} 2y + 3z &= 4 \\ 2x + 4y + 3z &= 5 \\ x+y &= 4
                \end{array} \]
    \end{enumerate}
\end{problem}
\hint{
    Remember that if $A \bx = \bb$ and $A$ is square then we first factor $A = LU$ and the
    system becomes $LU \bx = \bb$.  Defining $\by = U \bx$ we solve $L \by = \bb$ with a
    lower triangular solve and then
    solve $U \bx = \by$ with an upper triangular solve.

    If we use the QR factorization then $A = QR$ and the system becomes $QR\bx = \bb$.
    Recalling that $Q$ is an orthonormal matrix we only need to solve $R \bx = Q^T \bb$
    with an upper triangular solve.
}

\begin{problem}
    Find a least squares solution to the equation $A \bx = \bb$ in two different ways with 
    \[ A = \begin{pmatrix} 1 & 3 & 5 \\ 4 & -2 & 6 \\ 4 & 7 & 8 \\ 3 & 7 & 19
        \end{pmatrix} \quad \text{and} \quad \bb = \begin{pmatrix} 5 \\ 2 \\ -2 \\
        8\end{pmatrix}. \]
\end{problem}
\hint{
    This is a rectangular system so be careful with your implementation of algorithms.
    You can't do an LU factorization directly on $A$.
}

\begin{problem}
    Now that you have $QR$ and $LU$ code we're going to use both of them!  The problem is
    as follows: \\
    We are going to find the polynomial of degree 4 that best fits the function 
    \[ y =
    \cos(4t) + 0.1 \varepsilon(t) \]
    at 50 equally spaced points $t$ between $0$ and $1$.  Here
    we are using $\varepsilon(t)$ as a function that outputs normally distributed random
    white noise.  
    \ifnum\Python=0
    In \texttt{MATLAB} you will build $y$ as \\
    \mcode{y = cos(4*t) + 0.1*randn(size(t));}
    \else
    In Python you will build $y$ as \\
    \mcode{y = np.cos(4*t) + 0.1*np.random.randn(t.shape);}
    \fi
    
    Build the $t$ vector
    and the $y$ vector (these are your data).  We need to set up the least squares
    problems $A \bx = \bb$ by setting up the matrix $A$
    as we did in the other least squares curve fitting problems and by setting up the
    $\bb$ vector using the $y$ data you just built.  
    \begin{enumerate}
        \item[(a)] Solve the normal equations $A^T A \bx = A^T \bb$ using your $LU$ code.
        \item[(b)] Solve the system $A \bx = \bb$ by first transforming $A$ to $A = QR$
            and then solving $R\bx = Q^T \bb$.
        \item[(c)] Use \ProgLang to find the sum of the square errors between the
            polynomial approximation and the function $f(t) = \cos(4t)$ for both the $QR$
            and the $LU$ approaches.  
        \item[(d)] Build \ProgLang code that does parts (a) - (c) several hundred
            times and complies results comparing which method gives the better
            approximation (smaller sum of square error).
    \end{enumerate}
\end{problem}


% \begin{problem}
%     In this exercise we will use numerical linear algebra to do some handwriting
%     recognition on the classical data set \mcode{mnist}.  The \mcode{mnist} data set
%     contains a training set of 60,000 numbers and a test set of 10,000 numbers.  Each
%     digit in the database was placed in a 28 by 29 grayscale image such that the center of
%     mass of its pixels is at the center of the picture.  While our primary goal for this
%     problem is to use numerical linear algebra, our secondary goal is to get some
%     experience with the logic of machine learning.  In machine learning problems we often
%     follow the following logic:
%     \begin{itemize}
%         \item First use a set of data for which we know the {\it answer} (in this case the
%             {\it answer} is the numerical value of the digit that was written).  We call
%             this the training data set in the sense that we train the numerical method
%             with the correct answers in mind.
%         \item Next we compare data with hidden answers to our training set and build a
%             method for using the training set to make a prediction for the answer.  This
%             is called the testing phase of a machine learning algorithm.  In the testing
%             phase we know the {\it answer} but keep it hidden from the algorithm.  We let
%             our algorithm predict the {\it answer} and then compare to the hidden truth.
%             At the end of this step we can give a percent effectiveness for our algorithm.
%         \item In the final step of a machine learning process we give the numerical
%             algorithm data where we don't know the {\it answer} and use the algorithm to
%             predict for us.  
%     \end{itemize}
% 
%     Let's be more specific.  Let's say that we want to determine if a handwritten digit is
%     the number ``0''.  From the training set we average all of the zeros together to get a
%     best estimate of what a ``0'' looks like.  Then we build a mathematical technique for
%     doing the comparison between our new digit and the averaged training 0.  If the
%     comparison technique tells us that our new digit is {\it close enough} then we call
%     that new digit a zero. We can do this for all of the \mcode{test} 0's and determine
%     the percent effectiveness for our comparison technique. 
% 
%     {\bf Your Tasks:}
%     \begin{enumerate}
%         \item[(a)] Start by going to
%             \href{http://www.cs.nyu.edu/~roweis/data.html}{www.cs.nyu.edu/$\sim$roweis/data.html}
%             to download the data set.  Download the file titled \mcode{mnist_all.mat}.
%             This file is a MATLAB file that needs to be read into your working session of
%             MATLAB.  
%         \item[(b)] To read the \mcode{mnist_all.mat} data into MATLAB \\
%             \mcode{load mnist_all.mat}\\
%             Then type \mcode{whos} to see the variables containing training digits
%             (\texttt{train0, ..., train9}) and test digits (\texttt{test0, ...,
%             test9}).
%         \item[(c)] To visualize the first image in the matrix \texttt{train0} use 
% \begin{lstlisting}
% digit = train0(1,:); % read the first row all columns out of train0
% digitImage = reshape(digit,28,28); % turn into a 28x28 matrix
% image(rot90(flipup(digitImage),-1))
% colormap(gray(256))
% axis square tight off
% \end{lstlisting}
%         \item[(d)] Create a 10 by 784 matrix $T$ whose $i^{th}$ row contains the average
%             pixel values over all of the training images of the number $i-1$.  For
%             instance, the first row of $T$ can be formed by typing \\
%             \mcode{T(1,:) = mean(train0);} \\
%             Visualize these average digits using the \mcode{subplot} command creating a
%             $2 \times 5$ matrix of plots with the average 0 in the upper left and the
%             average 9 in the lower right.  Check yourself by making sure that your image
%             is identical to Figure \ref{fig:mnist_average}.
%         \item[(e)] We are going to try two methods for handwriting recognition.  There are
%             10,000 test numbers that are not in the training set and we want to try two
%             different ways of determining which digit is in the test image.  Your job is
%             to implement both of these methods.  You need to test all 10,000 test images and
%             gather statistics on how often the method identifies the test image.  Report
%             your answers by stating the proportion of correct identifications for each of
%             the 10 numerals.
%             \begin{description}
%                 \item[Method \#1 (Min Norm):] Compare pixels in the test digit to each row of the
%                     training matrix $T$ and determine which row most closely resembles the
%                     test digit.  Pseudo code for this method is:
%                     \begin{itemize}
%                         \item Let $D$ be the first test digit in \texttt{test0} using \\
%                             \mcode{D = double(test0(1,:));}
%                         \item For each row $i=1, 2, \cdots, 10$ compute \\
%                             \mcode{norm(T(i,:) - D)} \\
%                             and determine which value of $i$ this is smallest
%                         \item $D$ is probably the digit $i-1$.
%                         \item repeat for all of the test digits in \texttt{test0},
%                             \texttt{test1}, \ldots.
%                     \end{itemize}
%                     Be sure to write code to test all of the 10,000 test digits.  There
%                     are elegant ways to code this but you can also complete this task with
%                     a bunch of copy and paste.
%                 \item[Method \#2 (Min Projections):] Project the test image vector $D$
%                     onto the mean training image $T(i,:)$ and find the error in the
%                     projection.  In this method we seek to minimize the size of the error.
%                     Recall that the projection of $D$ onto $T(i,:)$ is 
%                     \[ \frac{D \cdot T(i,:)}{T(i,:) \cdot T(i,:)} \]
%                     and the error in the projection is 
%                     \[ \left\| D - \left( \frac{D \cdot T(i,:)}{T(i,:) \cdot T(i,:)} \right)
%                     T(i,:) \right\| \]
%             \end{description}
%     \end{enumerate}
% \end{problem}
% 
% \begin{figure}
%     \begin{center}
%         \includegraphics[width=0.9\columnwidth]{mnist_average_training_image.eps}
%     \end{center}
%     \caption{The 10 images are the averages of all of the training images for each digit.
%         They represent what a {\it typical} digit should look like for each of the 10
%     digits.}
%     \label{fig:mnist_average}
% \end{figure}
% 
% 


\begin{problem}
    Find the largest eigenvalue of the matrix $A$ WITHOUT using the built in
    ``\mcode{eig}'' command in \ProgLang.
    \[ A = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 0 & 1 & 2 \\ 3 & 4 & 5 &
        6 \end{pmatrix} \]
\end{problem}
\hint{
    You should be using the power method.
}


\begin{problem}
    Find a least square cubic function that best fits the following data. Solve this
    problem with Excel and with \ProgLang using the normal equations.
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            $x$ & $y$ \\\hline \hline
            0   & 1.0220\\
            0.0500&   1.0174\\
            0.1000&   1.0428\\
            0.1500&   1.0690\\
            0.2000&   1.0505\\
            0.2500&   1.0631\\
            0.3000&   1.0458\\
            0.3500&   1.0513\\
            0.4000&   1.0199\\
            0.4500&   1.0180\\
            0.5000&   1.0156\\
            0.5500&   0.9817\\
            0.6000&   0.9652\\
            0.6500&   0.9429\\
            0.7000&   0.9393\\
            0.7500&   0.9266\\
            0.8000&   0.8959\\
            0.8500&   0.9014\\
            0.9000&   0.8990\\
            0.9500&   0.9038\\
            1.0000&   0.8989 \\\hline
        \end{tabular}
    \end{center}
\end{problem}



\begin{thm}[Eigen-Structure of Symmetric Matrices]\label{thm:symmetric_matrix_thm}
    If $A$ is a symmetric matrix with eigenvalues $\lambda_1, \lambda_2, \ldots,
    \lambda_n$ then $|\lambda_1| > |\lambda_2| > \cdots > |\lambda_n|$.  Furthermore, the
    eigenvectors will be orthogonal to each other. 
\end{thm}


\begin{problem}
    For symmetric matrices we can build an extension to the Power Method in order
    to find the second most dominant eigen-pair for a matrix $A$.  Theorem
    \ref{thm:symmetric_matrix_thm} suggests the following method for finding the second
    dominant eigen-pair for a symmetric matrix.  This method is called the {\bf deflation
    method}.
    \begin{itemize}
        \item Use the power method to find the dominant eigenvalue and eigenvector.
        \item Start with a random unit vector of the correct shape.
        \item Multiplying your vector by $A$ will {\it pull it toward} the dominant
            eigenvector.  After you multiply, project your vector onto the dominant
            eigenvector and find the projection error.  
        \item Use the projection error as the new approximation for the eigenvector.
    \end{itemize}    

    Note that the deflation method is really exactly the same as the power method with the
    exception that we orthogonalize at every step.  Hence, when you write your code expect
    to only change a few lines from your Power method.

    Write a \ProgLang function \mcode{MyPower2} to find the second largest eigenvalue and
    eigenvector pair by putting the deflation method into practice. Test your code on a
    \underline{symmetric} matrix $A$ and compare against \ProgLang's \mcode{eig} command.
    Your code needs to work on symmetric matrices of arbitrary size and you need to write
    test code that clearly shows the error between your calculated eigenvalue and
    \ProgLang's
    eigenvalue as well as your calculated eigenvector and \ProgLang's eigenvector.\\ To
    guarantee that you start with a symmetric matrix you can use the following code.
    \ifnum\Python=0
\begin{lstlisting}
N = 40; % size of the matrix ... make this large-ish
A = rand(N,N);
A = A'*A; % this will be a random symmetric NxN matrix.
\end{lstlisting}
\else
\begin{lstlisting}
import numpy as np
N = 40
A = np.random.rand(N,N)
A = np.matrix(A)
A = np.transpose(A) * A
\end{lstlisting}
\fi
\end{problem}
\hint{
    Technically speaking you don't have to orthogonalize at every step so if you want to
    make your code more efficient you can orthogonalize every $k^{th}$ step (for your
    choice of $k$).  It would be cool to check what this does to the efficiency of your
    algorithm.
}





\newpage\section{Projects}
In this section we propose several ideas for projects related to numerical linear algebra.
These projects are meant to be open ended, to encourage creative mathematics, to push your
coding skills, and to require you to write and communicate your mathematics.  Take the
time to read Appendix \ref{app:writing_projects} before you write your final solution.

\subsection{Applications of the Singular Value Decomposition}
The singular value decomposition (SVD) is a matrix factorization that can be thought of as
the generalization of the eigenvalue problem for non-square matrices.  We wish to take a
matrix $A$ that has size $m \times n$ and factor it into three matrices, $U$, $\Sigma$,
and $V$ such that 
\[ A = U \Sigma V^T \]
where $U$ is an $m \times m$ orthonormal matrix, $\Sigma$ is an $m \times n$ rectangular
diagonal matrix, and $V$ is and $n \times n$ orthonormal matrix.  Recall that an
orthonormal matrix
is a matrix where the transpose is the inverse.  That is, $UU^T = I_{m \times m}$ and $VV^T
= I_{n \times n}$. The columns of $U$ are called the left singular vectors of $A$ and are
the eigenvectors of the matrix $AA^T$.  The columns of $V$ are called the right singular
vectors of $A$ and are the eigenvectors of the matrix $A^TA$.  The diagonal entries of
$\Sigma$ are the square roots of the non-zero eigenvalues of both $AA^T$ and $A^TA$.

It is probably best to have a look at an example (modified from
\footnote{\href{http://www.d.umn.edu/~mhampton/m4326svd_example.pdf}{http://www.d.umn.edu/~mhampton/m4326svd\_example.pdf}}).  Consider the matrix 
\[ A = \begin{pmatrix} 3 & 2 & 1 \\ 2 & 3 & -2 \end{pmatrix}. \]
We would like to factor $A$ into $A = U \Sigma V^T$.  First we find the eigenvalues and
eigenvectors of $AA^T$.  It is left to the reader to verify\footnote{That means that you
should stop what you're doing and actually verify this result!} that the eigenvalues are
$\lambda_1 = 25$ and $\lambda_2 = 9$.  Hence, the singular values are $\sigma_1 = 5$ and
$\sigma_2 = 3$.  Next we find the right singular vectors (the columns of $V$) by finding
an orthonormal set of eigenvectors of $A^TA$.  This too is left to the reader to verify
that the matrix $V$ is 
\[ V = \begin{pmatrix}
        1/\sqrt{2} & 1/\sqrt{18} & 2/3 \\
        1/\sqrt{2} & -1/\sqrt{18} & -2/3 \\
        0 & 4/\sqrt{18} & -1/3 
    \end{pmatrix}. \]
The reader should also observe that the columns of $V$ are all orthogonal to each other as
well as being unit vectors.

We now know that 
\[ A = U \Sigma V^T = U \begin{pmatrix} 5 & 0 & 0 \\ 0 & 3 & 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} & 0 \\ 1/\sqrt{18} & -1/\sqrt{18} &
        4/\sqrt{18} \\ 2/3 & -2/3 & -1/3 \end{pmatrix} \]
Since $V$ is an orthogonal matrix and the singular values $\sigma_1$ and $\sigma_2$ are
non-zero we can see that 
\[ U = AV\Sigma^\dagger \]
where 
\[ \Sigma^\dagger = \begin{pmatrix} 1/5 & 0 \\ 0 & 1/3 \\ 0 & 0 \end{pmatrix}. \]
After a bit of computation we finally find that 
\[ U = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix}
        \]
and hence
\[ A = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix} \begin{pmatrix} 5 & 0 & 0 \\ 0 & 3 & 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} & 0 \\ 1/\sqrt{18} & -1/\sqrt{18} &
        4/\sqrt{18} \\ 2/3 & -2/3 & -1/3 \end{pmatrix} \]
We leave it to the reader to check that this is correct.  

Obviously this is not a computation that you typically do by hand on larger matrices.  You
should have SVD code handy that does this entire algorithm.  \ProgLang also has a built-in
svd code that is optimized to run faster than the one that we built in class.  You are
welcome to use the built-in svd \ProgLang routine in this project.

\subsection*{Problem Statement}
There are many applications of the singular value decomposition.  In fact, the SVD is
considered one of the most useful and widely used matrix decompositions in the linear
algebra arsenal.  Your job for this project will be to explore two of those applications.
Below you will find a brief list of some of the primary uses for the singular value
decomposition.  Some of them are rather intricate and some are fairly straight forward to
implement, but I leave it to you to choose applications that are of interest to you.

\subsection*{Your Tasks}
You and your partner will choose two applications and do the following:
\begin{enumerate}
    \item[(a)] implement non-trivial examples of the application 
    \item[(b)] write a brief technical report detailing how each application works
        (including all of the mathematical details), and
    \item[(c)] write a 1 paragraph non-technical summary describing
        each application and how it works.  The audience for this non-technical summary is
        any reasonably educated, but not necessarily mathematically sophisticated,
        adult.
\end{enumerate}

Note: All code for this project must be uniquely yours or your partners.  

\subsection*{List of Applications}
Each of the following applications of the singular value decomposition is followed by a
brief description (paraphrased from various web sources linked in the footnotes).  Use
this to guide your choice.
\begin{description}
    \item[Principal Component Analysis:] (PCA) is a statistical tool that allows you to
        convert a set of possibly correlated data observations into uncorrelated
        components.  This is a method of data reduction that, in some sense, reduces how much
        data is actually influential in your data set.  PCA takes
        a potentially large data set and reduces it down to its {\it principal
        components} that are the most influential on the data.  PCA itself has many
        applications ranging from statistics to signal processing.  If you choose this
        application you will need to implement PCA on a non-trivial data set of your
        choosing.  There are several sample data sets on the web but the
        ones that are used for explanation are often quite trivial in size
        and scope.  \footnote{\href{https://en.wikipedia.org/wiki/Principal_component_analysis}{PCA
        Wiki page: en.wikipedia.org/wiki/Principal\_component\_analysis}}  Absolutely do
        not just make up your own data set.
    \item[Low Rank Image Approximations:] There are literally hundreds of ways to trim
        information from images while retaining the most important parts of the image.  One of the simplest is to apply the singular value decomposition to the
        rectangular image (either in grayscale or separately for each RBG matrix).  Then
        you can zero out the lower value singular values and vectors and then rebuild the
        image.  The
        low rank approximation (really just another type of data reduction) does obviously lead to some loss of information but up to a certain point
        this loss of information is un-noticeable to the human eye.  If you choose this
        application you need to supply your own images as well as give several examples of
        how low rank approximation changes the image.  You should also 
        explore how low rank approximations can be used as a method of image compression.  and give some sort of mathematical measure
        defining how much data reduction was performed along with the re-generated images
        so we can see the
        reduction.\footnote{\href{http://math.arizona.edu/~brio/VIGRE/ThursdayTalk.pdf}{A
        Sample of the Image Compression Technique: math.arizona.edu/\~brio/VIGRE/ThursdayTalk.pdf}}
    \item[Recommender Systems:] You run in the {\it recommender systems} all the time.
        Amazon seems to know what other books you might like, NetFlix seems to know what
        other movies you might like, and the list goes on.  One way to perform the
        matching behind the scenes in a
        recommender system is to use the SVD along with data reduction techniques (just
        like with PCA and image compression).  If you choose this application you need to
        provide a non-trivial example of a recommender system preferably of real data
        released from one of the large common sites (NetFlix, Amazon,
        etc.).\footnote{\href{http://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/svd.html}{The
            idea of SVD for Recommender Systems:
        http://www.cs.carleton.edu/cs\_comps/0607/recommend/recommender/svd.html}}
        \footnote{\href{http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf}{Paper
            about Recommender Systems:
        http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf}}
    \item[The Kabsh Algorithm:] This is an algorithm for calculating the optimal rotation
        matrix that minimizes the error between two sets of points.  This
        is a useful method in comparing molecular structures (including proteins) but is
        not just used in mathematical biology.  If you
        choose this application then you need to provide several non-trivial examples of
        how the method works and what it is that you are finding.
        \footnote{\href{https://en.wikipedia.org/wiki/Kabsch_algorithm}{Wiki page for
        Kabsh Algorithm: https://en.wikipedia.org/wiki/Kabsch\_algorithm} }
%     \item[Image Deblurring:] A method for taking a blurred image and returning it to the
%         sharp (or nearly sharp) original image is related to the singular value
%         decomposition.  This method requires you to know the way in which the image was
%         blurred but the inverse of the process is usually contaminated with noise so
%         taking a simple linear inverse neither works nor makes sense.  Follow the link in
%         the footnote for a full textbook on the image deblurring problem.
%         \footnote{\href{http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/ImageDeblurring_2006.pdf}{Image
%         Deblurring Text: http://web.ipac.caltech.edu/staff/fmasci/home/astro\_refs/ImageDeblurring\_2006.pdf}} 
    \item[Choose Your Own Adventure:] If you happen upon an interesting and non-trivial
        application of the singular value decomposition then you are welcome to use this
        as one of your two applications.  Just be sure that you can implement it 
        and that you can fully explain what you're doing. A decent place to start is the
        applications section of the SVD Wiki page.
        \footnote{\href{https://en.wikipedia.org/wiki/Singular_value_decomposition}{SVD
            Wiki:
        https://en.wikipedia.org/wiki/Singular\_value\_decomposition}}
\end{description}


\newpage\subsection{The Google Page Rank Algorithm}


In this project you will discover how the PageRank algorithm works to give the most relevant
information as the top hit on a Google search.  

Search engines compile large indexes of the dynamic information on the Internet so they
are easily searched.  This means that when you do a Google search, you are not actually
searching the Internet; instead, you are searching the indexes at Google.

When you type a query into Google the following two steps take place:
\begin{enumerate}
    \item Query Module: The query module at Google converts your natural language into a
        language that the search system can understand and consults the various indexes
        at Google in order to answer the query.  This is done to find the list of relevant
        pages.
    \item Ranking Module: The ranking module takes the set of relevant pages and ranks
        them. The outcome of the ranking is an ordered list of web pages such
        that the pages near the top of the list are most likely to be what you desire from
        your search. This ranking is the same as assigning a {\it popularity score} to
        each web site and then listing the relevant sites by this score.  
\end{enumerate}

This section focuses on the Linear Algebra behind the Ranking Module developed by the
founders of Google: Sergey Brin and Larry Page.  Their algorithm is called the
\emph{PageRank algorithm}, and you use it every single time you use Google's search
engine.


In simple terms: {\it A webpage is important if it is pointed to by other important
pages}.

The Internet can be viewed as a directed graph (look up this term
\href{https://en.wikipedia.org/wiki/Directed_graph}{here on Wikipedia}) where the nodes
are the web pages and the edges are the hyperlinks between the pages. The hyperlinks into a
page are called {\it inlinks}, and the ones pointing out of a page are called {\it
outlinks}.  In essence, a hyperlink from my page to yours is my endorsement of your page.
Thus, a page with more recommendations must be more important than a page with a few
links.  However, the status of the recommendation is also important. 

Let us now translate this into mathematics. To help understand
this we first consider the small web of six pages shown in Figure
\ref{fig:example_graph} (a graph of the router level of the internet can be found
\href{https://personalpages.manchester.ac.uk/staff/m.dodge/cybergeography/atlas/lumeta_large.jpg}{here}).  The links between the
pages are shown by arrows. An arrow pointing into a node is an {\it inlink}
and an arrow pointing out of a node is an {\it outlink}. In Figure
\ref{fig:example_graph}, node 3 has three outlinks (to nodes 1, 2, and 5)
and 1 inlink (from node 1).

\begin{figure}[ht]
    \begin{center}
        \begin{tikzpicture}
            \draw (0,0) node[circle,draw]{3};
            \draw (-1,1) node[circle,draw]{1};
            \draw (1,1) node[circle,draw]{2};
            \draw (-1,-1) node[circle,draw]{6};
            \draw (1,-1) node[circle,draw]{5};
            \draw (0,-2) node[circle,draw]{4};
        %
            \draw[<->] (-0.25,0.25) -- (-0.75,0.75);
            \draw[->] (-0.65,1) -- (0.65,1);
            \draw[->] (0.25,0.25) -- (0.75,0.75);
            \draw[->] (0.25,-0.25) -- (0.75,-0.75);
            \draw[->] (0.65,-1) -- (-0.65,-1);
            \draw[<->] (-0.75,-1.25) -- (-.25,-1.75);
            \draw[<->] (0.75,-1.25) -- (0.25,-1.75);
        \end{tikzpicture}
    \end{center}
        \caption{Sample graph of a web with six pages.}
        \label{fig:example_graph}
\end{figure}

We will first define some notation in the PageRank algorithm:
\begin{itemize}
    \item $|P_i|$ is the number of outlinks from page $P_i$
    \item $H$ is the {\it hyperlink} matrix defined as 
        \[ H_{ij} = \left\{ \begin{array}{cl} \frac{1}{|P_j|}, & \text{if there is a link
            from node $j$ to node $i$} \\ 0, & \text{otherwise} \end{array} \right. \]
        where the ``$i$'' and ``$j$'' are the row and column indices respectively.  
    \item $\bx$ is a vector that contains all of the PageRanks for the individual pages.
\end{itemize}

The PageRank algorithm works as follows:
\begin{enumerate}
    \item Initialize the page ranks to all be equal. This means that our initial
        assumption is that all pages are of equal rank.  In the case of Figure
        \ref{fig:example_graph} we would take $\bx_0$ to be 
        \[ \bx_0 = \begin{pmatrix} 1/6 \\ 1/6 \\ 1/6 \\ 1/6 \\ 1/6 \\ 1/6 \end{pmatrix}. \]
    \item Build the hyperlink matrix.  \\ As an example we'll consider node 3 in Figure
        \ref{fig:example_graph}.  There are three outlinks from node 3 (to nodes 1, 2, and
        5).  Hence $H_{13}=1/3$, $H_{23} = 1/3$, and $H_{53} = 1/3$ and the partially
        complete hyperlink matrix is
        \[ H = \begin{pmatrix} 
                - & - & 1/3 & - & - & - \\
                - & - & 1/3 & - & - & - \\
                - & - & 0   & - & - & - \\
                - & - & 0   & - & - & - \\
                - & - & 1/3 & - & - & - \\
                - & - & 0   & - & - & - 
            \end{pmatrix} \]
    \item The difference equation $\bx_{n+1} = H \bx_n$ is used to iteratively refine the
        estimates of the page ranks.  You can view the iterations as a person visiting a
        page and then following a link at random, then following a random link on the next
        page, and the next, and the next, etc.  Hence we see
        that the iterations evolve exactly as expected for a difference equation.
        \begin{center}
            \begin{tabular}{|c|c|}
                \hline
                Iteration & New Page Rank Estimation \\ \hline \hline
                0 & $\bx_0$ \\
                1 & $\bx_1 = H \bx_0$ \\
                2 & $\bx_2 = H \bx_1 = H^2 \bx_0$ \\
                3 & $\bx_3 = H \bx_2 = H^3 \bx_0$ \\
                4 & $\bx_4 = H \bx_3 = H^4 \bx_0$ \\
                \vdots & \qquad \vdots \\
                $k$ & $\bx_k = H^k \bx_0$ \\ \hline
            \end{tabular}
        \end{center}
    \item When a steady state is reached we sort the resulting vector $\bx_k$ to give the
        page rank. The node (web page) with the highest rank will be the top search
        result, the second highest rank will be the second search result, and so on.
\end{enumerate}

It doesn't take much to see that this process can be very time consuming.  Think about
your typical web search with hundreds of thousands of hits; that makes a square matrix $H$
that has a size of hundreds of thousands of entries by hundreds of thousands of entries!
The matrix multiplications alone would take many minutes (or possibly many hours) for
every search! \dots but Brin and Page were pretty smart dudes!!


We now state a few theorems and definitions that will help us simplify the iterative
PageRank process.
\begin{thm}\label{thm:eigen_expand}
    If $A$ is an $n \times n$ matrix with $n$ linearly independent eigenvectors $\bv_1,
    \bv_2, \bv_3,$ $\ldots, \bv_n$ and associated eigenvalues $\lambda_1, \lambda_2,
    \lambda_3, \ldots, \lambda_n$ then for any initial vector $\bx \in \mathbb{R}^n$ we
    can write $A^k \bx$ as
    \[ A^k \bx = c_1 \lambda_1^k \bv_1 + c_2 \lambda_2^k \bv_2 + c_3 \lambda_3^k \bv_3 +
        \cdots c_n \lambda_n^k \bv_n \]
    where $c_1, c_2, c_3, \ldots, c_n$ are the constants found by expressing $\bx$ as a
    linear combination of the eigenvectors. \\Note: We can assume that the eigenvalues are ordered
    such that $\lambda_1 \ge \lambda_2 \ge \lambda_3 \ge \cdots \ge \lambda_n$.
\end{thm}
\begin{proof}
    (Prove the preceding theorem)
\end{proof}

\begin{definition}
    A {\bf probability vector} is a vector with entries on the interval $[0,1]$ that add up to 1. 
\end{definition}
\begin{definition}
    A {\bf stochastic matrix} is a square matrix whose columns are probability vectors.
\end{definition}

\begin{thm} \label{thm:largest_ev_stochastic}
    If $A$ is a stochastic $n \times n$ matrix then $A$ will have $n$ linearly independent
    eigenvectors.  Furthermore, the largest eigenvalue of a stochastic matrix will
    \underline{always} be $\lambda_1 = 1$ and the smallest eigenvalue  will always be
    nonnegative: $0 \le \lambda_n < 1$.
\end{thm}

Some of the following tasks will ask you to {\it prove} a statement or a theorem.  This
means to clearly write all of the logical and mathematical reasons why the statement is
true. Your proof should be absolutely crystal clear to anyone with a similar mathematical
background \dots if you are in doubt then have a peer from a different group read your
proof to you \underline{out loud}.

\begin{problem}
    Finish writing the hyperlink matrix $H$ from Figure \ref{fig:example_graph}.
\end{problem}

\begin{problem}
    Write \ProgLang code to implement the iterative process defined previously. Make a plot
    that shows how the rank evolves over the iterations.
\end{problem}


\begin{problem}
    What must be true about a collection of $n$ pages such that an $n\times n$
        hyperlink matrix $H$ is a stochastic matrix.
\end{problem}

The statement of the next theorem is incomplete, but the proof is given to you.  Fill in
the blank in the statement of the theorem and provide a few sentences supporting your
answer.
\begin{thm}\label{thm:steady}
    If $A$ is an $n \times n$ stochastic matrix and $\bx_0$ is some initial vector
    for the difference equation $\bx_{n+1} = A \bx_n$, then the steady state
    vector is
    \[ \bx_{equilib} = \lim_{k \to\infty} A^k \bx_0 = \underline{\hspace{1in}}. \]
\end{thm}
\begin{proof}
    First note that $A$ is an $n \times n$ stochastic matrix so from Theorem
    \ref{thm:largest_ev_stochastic} we know that there are $n$ linearly
    independent eigenvectors.  We can then substitute
    the eigenvalues from Theorem \ref{thm:largest_ev_stochastic} in Theorem
    \ref{thm:eigen_expand}. Noting that if $0<\lambda_j<1$ we have $\lim_{k \to
    \infty} \lambda_j^k = 0$ the result follows immediately.
\end{proof}
\begin{problem}
    Discuss how Theorem \ref{thm:steady} greatly simplifies the PageRank iterative process
    described previously.  In other words: there is no reason to iterate at all.  Instead,
    just find \underline{\hspace{1in}}.
\end{problem}
\begin{problem}
\item Now use the previous two problems to find the resulting PageRank vector from the web in Figure
    \ref{fig:example_graph}?  Be sure to rank the pages in order of importance.
    Compare your answer to the one that you got in problem 2.
\end{problem}


\begin{problem}
    Consider the web in Figure \ref{fig:graph2}.
        \begin{enumerate}
            \item[(a)] Write the $H$ matrix and find the initial state $\bx_0$, 
            \item[(b)] Find
                steady state PageRank vector using the two different methods described:
                one using the iterative difference equation and the other using Theorem
                \ref{thm:steady} and the dominant eigenvector.
            \item[(c)] Rank the pages in order of importance.
        \end{enumerate}
\end{problem}
\begin{figure}[ht!]
    \begin{center}
        \begin{tikzpicture}
            \draw (0,0) node[circle,draw]{3};
            \draw (-1,1) node[circle,draw]{1};
            \draw (1,1) node[circle,draw]{2};
            \draw (-1,-1) node[circle,draw]{6};
            \draw (1,-1) node[circle,draw]{5};
            \draw (0,-2) node[circle,draw]{4};
            \draw (-2,0) node[circle,draw]{7};
            \draw (-2,-2) node[circle,draw]{8};
        %
            \draw[<-] (-0.25,0.25) -- (-0.75,0.75);
            \draw[->] (-0.65,1) -- (0.65,1);
            \draw[<->] (0.25,0.25) -- (0.75,0.75);
            \draw[<->] (0.25,-0.25) -- (0.75,-0.75);
            \draw[->] (0.65,-1) -- (-0.65,-1);
            \draw[<->] (-0.75,-1.25) -- (-.25,-1.75);
            \draw[<->] (0.75,-1.25) -- (0.25,-1.75);
            \draw[<->] (-1.75,0.25) -- (-1.25,0.75);
            \draw[<->] (-1.65,0) -- (-0.35,0);
            \draw[<-] (-1.75,-0.25) -- (0.7,-0.8);
            \draw[->] (-2,-0.35) -- (-2,-1.65);
            \draw[<->] (-1.65,-2) -- (-0.35,-2);
        \end{tikzpicture}
    \end{center}
    \caption{Graph of a web with eight pages.}
    \label{fig:graph2}
\end{figure}


\begin{problem}
    One thing that we didn't consider in this version of the Google Page Rank algorithm is
    the random behavior of humans.  One, admittedly slightly naive, modification that we
    can make to the present algorithm is to assume that the person surfing the web will
    randomly jump to any other page in the web at any time.  For example, if someone is on
    page 1 in Figure \ref{fig:graph2} then they could randomly jump to any page 2 - 8.
    They also have links to pages 2, 3, and 7.  That is a total of 10 possible next steps
    for the web surfer.  There is a $2/10$ chance of heading to page 2.  One of those is
    following the link from page 1 to page 2 and the other is a random jump to page 2
    without following the link.  Similarly, there is a $2/10$ chance of
    heading to page 3, $2/10$ chance of heading to page 7, and a $1/10$ chance of randomly
    heading to any other page.

    Implement this new algorithm, called the {\it random surfer algorithm}, on the web in
    Figure \ref{fig:graph2}.  Compare your ranking to the non-random surfer results from
    the previous problem.
\end{problem}



% \newpage\subsection{Principal Component Analysis (Incomplete)}
